<?xml version="1.0" encoding="UTF-8"?>
<section xml:id="s-The Diagonalization Process">
<title>The Diagonalization Process</title>
<index><main>The Diagonalization Process</main></index>

<p>We now have the background to understand the main ideas behind the diagonalization process.</p>

<definition xml:id="def-eigenvalue-eigenvector"><title>Eigenvalue, Eigenvector</title>
<index><main>Eigenvalue</main></index>
<index><main>Eigenvector</main></index>
<statement><p>
 Let <m>A</m> be an \(n\times n\) matrix over <m>\mathbb{R}</m>.  \(\lambda\)  is an eigenvalue of \(A\) if for some nonzero column vector  \(\vect{x}\in \mathbb{R}^n\) we have \(A \vect{x} = \lambda  \vect{x}\).  \(\vect{x}\) is called an <term>eigenvector</term>
corresponding to the <term>eigenvalue</term> \(\lambda\).</p></statement></definition>




<example xml:id="ex-some-evalues"><title>Examples of eigenvalues and eigenvectors</title><p>Find the eigenvalues and corresponding eigenvectors of the matrix \(A=\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\).</p>
<p>We want to find nonzero vectors  \(\vect{x} = \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)\) and real numbers \lambda  such that
<mdn xml:id="equation-evalue-logic">
<mrow>
\begin{split}
A X = \lambda  X \quad &amp;\Leftrightarrow \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) = \lambda  \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)\\
 	&amp; \Leftrightarrow \text \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow  \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{cc}
			 1 &amp; 0 \\
			 0 &amp; 1 \\
			\end{array}
			\right) \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow  \left( \left(
			\begin{array}{cc}
			 2 &amp; 1 \\
			 2 &amp; 3 \\
			\end{array}
			\right) - \lambda  \left(
			\begin{array}{cc}
			 1 &amp; 0 \\
			 0 &amp; 1 \\
			\end{array}
			\right) \right)\left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
	&amp; \Leftrightarrow \text \left(
			\begin{array}{cc}
			 2-\lambda  &amp; 1 \\
			 2 &amp; 3-\lambda  \\
			\end{array}
			\right) \left(
			\begin{array}{c}
			 x_1 \\
			 x_2 \\
			\end{array}
			\right)=\left(
			\begin{array}{c}
			 0 \\
			 0 \\
			\end{array}
			\right)\\
\end{split}
</mrow>
</mdn>
</p>

<p>The last matrix equation will have nonzero solutions if and only if



\[\det  \left(
\begin{array}{cc}
 2-\lambda  &amp; 1 \\
 2 &amp; 3-\lambda  \\
\end{array}
\right) =0\]



or  \((2 - \lambda )(3 -\lambda ) - 2 = 0\), which simplifies to \(\lambda^2 - 5\lambda  + 4 = 0\).   Therefore, the solutions to this quadratic equation, \(\lambda_1 = 1\) and \(\lambda_2 = 4\), are the eigenvalues of <m>A</m>. We now have to find eigenvectors associated with each eigenvalue.</p>

<p>Case 1. For \(\lambda_1= 1\),  <xref ref="equation-evalue-logic" autoname="yes" />  becomes:

\[\left(
\begin{array}{cc}
 2-1 &amp; 1 \\
 2 &amp; 3-1 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right) \\
\\
\left(
\begin{array}{cc}
 1 &amp; 1 \\
 2 &amp; 2 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right)\textrm{    }\text \]



which reduces to the single equation, \(x_1+ x_2= 0\).  From this, \(x_1= -x_2\). This means the solution set of this equation is (in column notation)



\[E_1 = \left\{ \left.\left(
\begin{array}{c}
 -c \\
 c \\
\end{array}
\right) \right| c\in  \mathbb{R}\right\}\]



So any column vector of the form \(\left(
\begin{array}{c}
 -c \\
 c \\
\end{array}
\right)\) where <m>c</m> is any nonzero real number is an eigenvector associated with  \(\lambda_1=1\).   The reader should verify that,
for example, 

\[\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right) = 1 \left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right)\]



 so that \(\left(
\begin{array}{c}
 \frac{2}{3} \\
 -\frac{2}{3} \\
\end{array}
\right)\) is an eigenvector associated with eigenvalue 1.</p>



<p>Case 2.  For \(\lambda_2=4\) <xref ref="equation-evalue-logic" autoname="yes" /> becomes:

\[\left(
\begin{array}{cc}
 2-4 &amp; 1 \\
 2 &amp; 3-4 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right) \\
\\
\left(
\begin{array}{cc}
 -2 &amp; 1 \\
 2 &amp; -1 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 0 \\
\end{array}
\right)\textrm{    }\text \]



which reduces to the single equation \(-2x_1+x_2=0\), so that \(x_2= 2x_1\). The solution set of the equation is



\[E_2=\left\{\left.\left(
\begin{array}{c}
 c \\
 -2c \\
\end{array}
\right)\right| c\in  \mathbb{R}\right\}\]



Therefore, all eigenvectors of <m>A</m> associated with the eigenvalue \(\lambda_2 = 4\) are of the form \(\left(
\begin{array}{c}
 c \\
 -2c  \\
\end{array}
\right)\), where <m>c</m> can be any nonzero number.

</p></example>

The following theorems summarize the more important aspects of <xref ref="ex-some-evalues" autoname="yes" />:

<theorem xml:id="theorem-evalue-det-theorem"><title>Characterization of Eigenvalues of a Square Matrix</title>
<index><main>12.4.1.</main></index>
<statement>Let <m>A</m> be any \(n\times n\) matrix over \(\mathbb{R}\). Then \(\lambda \in  \mathbb{R}\) is an eigenvalue of \textit{
A} if and only if \(\det (A - \lambda  I) = 0\).</statement>
<proof></proof></theorem> 

<p>The equation \(\det (A - \lambda  I) = 0\) is called the <term>characteristic equation</term>, and the left side of this equation is called the <term>
characteristic polynomial</term> of <m>A</m>.</p>

<theorem xml:id="theorem-evector-independence"><title>Linear Independence of Eigenvectors</title><statement>Nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent.</statement></theorem>

<p>The solution space of \((A-\lambda  I)\vect{x}=\vect{0}\) is called the eigenspace of A corresponding to \(\lambda\). This terminology is justified by Exercise 2 of this section.</p>

<p>We now consider the main aim of this section. Given an \(n\times n\) (square) matrix <m>A</m>, we would like to transform <m>A</m> into a diagonal matrix <m>D</m>, perform our tasks with the simpler matrix <m>D</m>, and then describe the results in terms of the given matrix
A.</p>
<definition xml:id="def-diagonalizable-matrix">
<title>Diagonalizable Matrix.</title>
<index><main>Diagonalizable Matrix</main></index>
<statement><p>An \(n\times n\) matrix <m>A</m> is called diagonalizable if there exists an invertible \(n\times n\)
matrix <m>P</m> such that \(P^{-1} A P\) is a diagonal matrix <m>D</m>. The matrix <m>P</m> is said to diagonalize the matrix <m>A</m>.</p></statement></definition>
<example xml:id="ex-diagonalization-of-matrix">
<title>Diagonalization of a Matrix</title>
<p>We will now diagonalize the matrix <m>A</m> of <xref ref="ex-some-evalues" autoname="yes" />.  We form the matrix <m>P</m> as follows: Let \(P^{(1)}\)
be the first column of <m>P</m>.  Choose for \(P^{(1)}\) any eigenvector from \(E_1\). We may as well choose a simple vector in \(E_1\) so \(P^{(1)}=\left(
\begin{array}{c}
 1 \\
 -1 \\
\end{array}
\right)\) is our candidate.</p>
<p>Similarly, let \(P^{(2)}\)  be the second column of <m>P</m>, and choose for \(P^{(2)}\) any eigenvector from \(E_2\). The vector \(P^{(2)}=\left(
\begin{array}{c}
 1 \\
 2 \\
\end{array}
\right)\)  is a  reasonable choice, thus 



\[P= \left(
\begin{array}{cc}
 1 &amp; 1 \\
 -1 &amp; 2 \\
\end{array}
\right)\)    and    \(P^{-1}= \frac{1}{3}\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 1 \\
\end{array}
\right)=\left(
\begin{array}{cc}
 \frac{2}{3} &amp; -\frac{1}{3} \\
 \frac{1}{3} &amp; \frac{1}{3} \\
\end{array}
\right)\]



So that   



\[P^{-1} A P = \frac{1}{3}\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 1 \\
\end{array}
\right)\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{cc}
 1 &amp; 1 \\
 -1 &amp; 2 \\
\end{array}
\right) = \left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\]



Notice that the elements on the main diagonal of <m>D</m> are the eigenvalues of <m>A</m>, where \(D_{i i}\) is the eigenvalue corresponding
to the eigenvector \(P^{(i)}\) .
</p></example>
<note><p><ol label=“1”>
<li><p> The first step in the diagonalization process is the determination of the eigenvalues. The ordering of the eigenvalues is purely arbitrary.
If we designate \(\lambda_1 = 4\)  and \(\lambda_2=1\), the columns of P would be interchanged and <m>D</m> would be \(\left(
\begin{array}{cc}
 4 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right)\) (see Exercise 3b of this section).  Nonetheless, the final outcome of the application to which we are applying the diagonalization process
would be the same.</p></li>
<li><p> If <m>A</m> is an \(n\times n\) matrix with distinct eigenvalues, then <m>P</m> is also an \(n\times n\) matrix whose columns \(P^{(1)}\),
\(P^{(2)}, \ldots\), \(P^{(n)}\) are <m>n</m> linearly independent vectors.</p></li>
</ol>
</p>
</note>

<example xml:id="ex-another-diagonalization"><title>Diagonalization of a 3 by 3 matrix</title><p> Diagonalize the matrix
\(A= \left(
\begin{array}{ccc}
 1 &amp; 12 &amp; -18 \\
 0 &amp; -11 &amp; 18 \\
 0 &amp; -6 &amp; 10 \\
\end{array}
\right)\).</p>



<p>First, we find the eigenvalues of \(A\). 
<me>
\begin{split}
\det (A-\lambda  I) &amp;=\det \left(
			\begin{array}{ccc}
			 1-\lambda  &amp; 12 &amp; -18 \\
			 0 &amp; -\lambda -11 &amp; 18 \\
			 0 &amp; -6 &amp; 10-\lambda  \\
			\end{array}
			\right)\\
	&amp;=(1-\lambda ) \det  \left(
			\begin{array}{cc}
			 -\lambda -11 &amp; 18 \\
			 -6 &amp; 10-\lambda  \\
			\end{array}
			\right)\\
	&amp;=(1-\lambda ) ((-\lambda -11)(10-\lambda )+108) = (1-\lambda ) \left(\lambda ^2+\lambda -2\right)
\end{split}
</me>

Hence, the equation \(\det (A-\lambda  I)\) becomes
\[(1-\lambda ) \left(\lambda ^2+\lambda -2\right) =- (\lambda -1)^2(\lambda +2)\]

Therefore, our eigenvalues for <m>A</m> are \(\lambda_1= -2\) and \(\lambda_2=1\). We note that we do not have three distinct eigenvalues, but
we proceed as in the previous example.</p>

<p>Case 1.  For \(\lambda_1= -2\) the equation \((A-\lambda  I)\vect{x}= \vect{0}\)  becomes



 \[\left(
\begin{array}{ccc}
 3 &amp; 12 &amp; -18 \\
 0 &amp; -9 &amp; 18 \\
 0 &amp; -6 &amp; 12 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]



We can row reduce the matrix of coefficients to \(\left(
\begin{array}{ccc}
 1 &amp; 0 &amp; 2 \\
 0 &amp; 1 &amp; -2 \\
 0 &amp; 0 &amp; 0 \\
\end{array}
\right)\).</p>

<p>In equation form, the matrix equation is then equivalent to 
\(x_1 = -2x_3\\
\\
x_2= 2x_3\).  Therefore, the solution, or eigenspace, corresponding to \(\lambda_1=-2\) consists of vectors of the form 
\[\left(
\begin{array}{c}
 -2x_3 \\
 2x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 -2 \\
 2 \\
 1 \\
\end{array}
\right)\]
</p>


<p>Therefore \(\left(
\begin{array}{c}
 -2 \\
 2 \\
 1 \\
\end{array}
\right)\) is an eigenvector corresponding to the eigenvalue \(\lambda_1=-2\), and can be used for our first column of \(P\):



\(P= \left(
\begin{array}{ccc}
 -2 &amp; ? &amp; ? \\
 2 &amp; ? &amp; ? \\
 1 &amp; ? &amp; ? \\
\end{array}
\right)\)
</p>

<p>Before we continue we make the observation: \(E_2\) is a subspace of \(\mathbb{R}^3\) with basis \(\left\{P^{(1)}\right\}\) and  \(\dim  E_1 =
1\).</p>



<p>Case 2. If \(\lambda_2= 1\), then the equation \((A-\lambda  I)\vect{x}= \vect{0}\) becomes



 \[\left(
\begin{array}{ccc}
 0 &amp; 12 &amp; -18 \\
 0 &amp; -12 &amp; 18 \\
 0 &amp; -6 &amp; 9 \\
\end{array}
\right) \left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\]
</p>

<p>Without the aid of any computer technology, it should be clear that all three equations that correspond to this matrix equation are equivalent to
 \(2 x_2-3x_3= 0\), or \(x_2= \frac{3}{2}x_3\).    Notice that \(x_1\) can take on any value, so any vector of the form



\[\left(
\begin{array}{c}
 x_1 \\
 \frac{3}{2}x_3 \\
 x_3 \\
\end{array}
\right)=x_1\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right)+x_3\left(
\begin{array}{c}
 0 \\
 \frac{3}{2} \\
 1 \\
\end{array}
\right)\]



will solve the matrix equation.</p>



<p>We note that the solution set contains two independent variables, \(x_1\) and \(x_3\). Further, note that we cannot express the eigenspace \(E_2\)
as a linear combination of a single vector as in Case 1.   However, it can be written as 



 \[E_2= \left\{x_1\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right)+x_3\left(
\begin{array}{c}
 0 \\
 \frac{3}{2} \\
 1 \\
\end{array}
\right) |x_1,x_3\in  \mathbb{R}\right\}.\]
</p>



<p>We can replace any vector in a basis is with a nonzero multiple of that vector.  Simply for aesthetic reasons, we will multiply the second vector
that generates \(E_2\) by 2.  Therefore, the eigenspace \(E_2\) is a subspace of \(\mathbb{R}^3\) with basis \(\left\{\left(
\begin{array}{c}
 1 \\
 0 \\
 0 \\
\end{array}
\right),\left(
\begin{array}{c}
 0 \\
 3 \\
 2 \\
\end{array}
\right)\right\}\) and so  \(\dim  E_2 = 2\).</p>


<p>What this means with respect to the diagonalization process is that \(\lambda_2= 1\)  gives us both Column 2 and Column 3 the diagonalizing matrix.
  The order is not important so we have 


\[P= \left(
\begin{array}{ccc}
 -2 &amp; 1 &amp; 0 \\
 2 &amp; 0 &amp; 3 \\
 1 &amp; 0 &amp; 2 \\
\end{array}
\right)\]</p>



<p>The reader can verify (see Exercise 5 of this section) that



 \(P^{-1}= \left(
\begin{array}{ccc}
 0 &amp; 2 &amp; -3 \\
 1 &amp; 4 &amp; -6 \\
 0 &amp; -1 &amp; 2 \\
\end{array}
\right)\)   and   \(P^{-1}A P = \left(
\begin{array}{ccc}
 -2 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 1 \\
\end{array}
\right)\)
</p></example>


<p>In doing <xref ref="ex-another-diagonalization" autoname="yes" />, the given \(3\times 3\) matrix <m>A</m> produced only two, not three, distinct eigenvalues, yet we were still able to
diagonalize <m>A</m>. The reason we were able to do so was because we were able to find three linearly independent eigenvectors. Again, the main
idea is to produce a matrix \(P\) that does the diagonalizing. If <m>A</m> is an \(n \times  n\) matrix, <m>P</m> will be an \(n\times
n\) matrix, and its <m>n</m> columns must be linearly independent eigenvectors. The main question in the study of diagonalizability is <q>When
can it be done?</q> This is summarized in the following theorem.

<theorem xml:id="theorem-12.4.3."><title>12.4.3.</title><index><main>12.4.3.</main></index><statement><p> Let \(A\) be an \(n \times  n\) matrix.   Then \(A\) is diagonalizable if and only if \(A\) has \(n\) linearly independent eigenvectors.</p></statement>
<proof><p>Outline of a proof: (\(\Longleftarrow\)) Assume that <m>A</m> has linearly independent eigenvectors, \(P^{(1)}\), \(P^{(2)}\), $\ldots $, \(P^{(n)}\),
with corresponding eigenvalues \(\lambda_1\), \(\lambda_2, \ldots\) , \(\lambda _n\).   We want to prove that <m>A</m> is diagonalizable.
Column <m>i</m> of the \(n \times n\) matrix \(A P\) is  \(A P^{(i)}\) (see Exercise 7 of this section). Then, since the \(P^{(i)}\) is an eigenvector
of <m>A</m>  associated  with the eigenvalue \(\lambda _i\) we have \(A P^{(i)}= \lambda _iP^{(i)}\) for\(i = 1, 2, \dots , n\). But this
means that \(A P = P D\), where \(D\) is the diagonal matrix with diagonal entries  \(\lambda_1\), \(\lambda_2,\ldots \), \(\lambda_n\).   If we multiply both sides of the equation by \(P^{-1}\) we get the desired \(P^{-1}A P = D\).</p>



<p>(\(\Longrightarrow \)) The proof in this direction involves a concept that is not covered in this text (rank of a matrix); so we refer the interested
reader to virtually any linear algebra text for a proof.  
</p></proof></theorem>

<p>We now give an example of a matrix which is not diagonalizable.</p>

<example xml:id="ex-not-diagonalizable">
<title>A Matrix that is Not Diagonalizable</title>
<p> Let us attempt to diagonalize the matrix \(A = \left(
\begin{array}{ccc}
 1 &amp; 0 &amp; 0 \\
 0 &amp; 2 &amp; 1 \\
 1 &amp; -1 &amp; 4 \\
\end{array}
\right)\)</p>


<p>First, we determine the eigenvalues.

<me>
\begin{split}
\det (A-\lambda  I) &amp;= \det  \left(
			\begin{array}{ccc}
			 1-\lambda  &amp; 0 &amp; 0 \\
			 0 &amp; 2-\lambda  &amp; 1 \\
			 1 &amp; -1 &amp; 4-\lambda  \\
			\end{array}
			\right)\\
		&amp;= (1-\lambda) \det \left(
			\begin{array}{cc}
			 2-\lambda  &amp; 1 \\
			 -1 &amp; 4-\lambda  \\
			\end{array}
			\right)\\
	&amp; = (1-\lambda )((2-\lambda )(4-\lambda )+1)\\
	&amp; = (1-\lambda )\left(\lambda ^2-6\lambda  +9\right)\\
	&amp; = (1-\lambda)(\lambda -3)^2 
\end{split}
</me>

Therefore there are two eigenvalues, \(\lambda_1= 1\) and \(\lambda_2=3\).  Since \(\lambda_1\) is an eigenvalue of degree it will have an eigenspace
of dimension 1.  Since \(\lambda_2\) is a double root of the characteristic equation, the dimension of its eigenspace must be 2 in order to be
able to diagonalize.



Case 1. For \(\lambda_1= 1\),  the equation \((A-\lambda  I)\vect{x} = \vect{0}\) becomes







\(\left(
\begin{array}{ccc}
 0 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 1 \\
 1 &amp; -1 &amp; 3 \\
\end{array}
\right)\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\)



A quick Mathematica.  evaluation make the solution to this system obvious

\begin{doublespace}
\noindent\(\pmb{\textrm{ RowReduce}[A-\textrm{ IdentityMatrix}[3]]}\)
\end{doublespace}



There is one free variable, \(x_3\), and 



\(\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)=\left(
\begin{array}{c}
 -4x_3 \\
 -x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 -4 \\
 -1 \\
 1 \\
\end{array}
\right)\)



Hence,  \(\left\{\left(
\begin{array}{c}
 -4 \\
 -1 \\
 1 \\
\end{array}
\right)\right\}\) is a basis for the eigenspace of \(\lambda_1= 1\).  



Case 2.  For \(\lambda_2= 3\),  the equation \((A-\lambda  I)\vect{x} = \vect{0}\) becomes



  \(\left(
\begin{array}{ccc}
 -2 &amp; 0 &amp; 0 \\
 0 &amp; -1 &amp; 1 \\
 1 &amp; -1 &amp; 1 \\
\end{array}
\right)\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)= \left(
\begin{array}{c}
 0 \\
 0 \\
 0 \\
\end{array}
\right)\)

\begin{doublespace}
\noindent\(\pmb{\textrm{ RowReduce}[A-3\textrm{ IdentityMatrix}[3]]}\)
\end{doublespace}



Once again there is only one free variable in the row reduction and so the dimension of the eigenspace will be one:



 \(\left(
\begin{array}{c}
 x_1 \\
 x_2 \\
 x_3 \\
\end{array}
\right)=\left(
\begin{array}{c}
 0 \\
 x_3 \\
 x_3 \\
\end{array}
\right)= x_3\left(
\begin{array}{c}
 0 \\
 1 \\
 1 \\
\end{array}
\right)\)



Hence,  \(\left\{\left(
\begin{array}{c}
 0 \\
 1 \\
 1 \\
\end{array}
\right)\right\}\) is a basis for the eigenspace of \(\lambda_2= 3\).  This means that \(\lambda_2= 3\) produces only one column for \textit{
P}. Since we began with only two eigenvalues, we had hoped that one of them would produce a vector space of dimension two, or, in matrix terms, two
linearly independent columns of <m>P</m>. Since <m>A</m> does not have three linearly independent eigenvectors  <m>A</m> cannot be diagonalized.</p></example>

<!-- 
<subsubsection xml:id="sss-\(\) { "><title>\(\) { </title><index><main>\(\) { </main></index>Mathematica.  Note}



Diagonalization can be easily done with a few built-in functions of Mathematica. .   Here is a \(3 \times  3\) matrix we've selected because
the eigenvalues are very simple, and could be found by hand with a little work.

\begin{doublespace}
\noindent\(\pmb{A=\left(
\begin{array}{ccc}
 4 &amp; 1 &amp; 0 \\
 1 &amp; 5 &amp; 1 \\
 0 &amp; 1 &amp; 4 \\
\end{array}
\right);}\)
\end{doublespace}



The set of linearly independent eigenvectors of A can be computed: 

\begin{doublespace}
\noindent\(\pmb{\textrm{ Eigenvectors}[A]}\)
\end{doublespace}



The rows of this matrix are the eigenvectors, so we transpose the result to get our diagonalizing matrix <m>P</m>. whose columns are eigenvectors.

\begin{doublespace}
\noindent\(\pmb{P=\textrm{ Transpose}[\textrm{ Eigenvectors}[A]]}\)
\end{doublespace}



We then use <m>P</m> to diagonalize.  The entries in the diagonal matrix are the eigenvalues of <m>A</m>.

\begin{doublespace}
\noindent\(\pmb{\textrm{ Inverse}[P].A.P}\)
\end{doublespace}



We could have gotten the eigenvalues directly this way:

\begin{doublespace}
\noindent\(\pmb{\textrm{ Eigenvalues}[A]}\)
\end{doublespace}



Most matrices that are selected at random will not have <q>nice</q> eigenvalues.   Here is a new matrix<m>A</m> that looks similar to the
one above.

\begin{doublespace}
\noindent\(\pmb{A=\left(
\begin{array}{ccc}
 8 &amp; 1 &amp; 0 \\
 1 &amp; 5 &amp; 1 \\
 0 &amp; 1 &amp; 7 \\
\end{array}
\right);}\)
\end{doublespace}



Asking for the eigenvalues first, we see that the result is returned symbolically as the three roots to a cubic equation.  The default for \textit{
Mathematica} is to leave these non-computed.  Since the entries of <m>A</m> are exact numbers, Mathematica.  is capable of giving an
exact solution, but it's very messy.   The easiest way around the problem is to make the entries in <m>A</m> approximate.  The following expression
redefines <m>A</m> as approximate. 

\begin{doublespace}
\noindent\(\pmb{A=N[A]}\)
\end{doublespace}



Now we can get approximate eigenvalues, and the approximations are very good for most purposes.

\begin{doublespace}
\noindent\(\pmb{\textrm{ Eigenvalues}[A]}\)
\end{doublespace}



We can verify that the matrix can be diagonalized although due to round-off error some of the off-diagonal entries of the <q>diagonal</q> matrix
are nonzero.

\begin{doublespace}
\noindent\(\pmb{P=\textrm{ Transpose}[\textrm{ Eigenvectors}[A]]}\)
\end{doublespace}

\begin{doublespace}
\noindent\(\pmb{\textrm{ Inverse}[P].A.P}\)
\end{doublespace}



The \(Chop\) function will set small numbers to zero.  The default thresh hold for <q>small</q> is \(10^{-10}\) but that can be adjusted, if desired.

\begin{doublespace}
\noindent\(\pmb{\textrm{ Diag}=\textrm{ Chop}[\textrm{ Inverse}[P].A.P]}\)
\end{doublespace}

\small{We can't use the name \(D\) here because Mathematica.  reserves it for the differentiation function.}



If you experiment with more matrices, you will undoubtedly encounter situations where some eigenvalues are complex.  The process is the same, although
we've avoided these just for simplicity.
 -->



<subsubsection xml:id="sss-\(\) { "><title>\(\) { </title><index><main>\(\) { </main></index>Sage Note}



We start by defining the same matrix as we did in Mathematica. .  We also declare \(D\) and \(P\) to be variables.

A = Matrix (QQ, [[4, 1, 0], [1, 5, 1], [0, 1, 4]]);A\\
[4 1 0]\\
[1 5 1]\\
[0 1 4]\\
var (' D, P')\\
(D, P)



We have been working with <q>right eigenvectors</q> since the \pmb{ <m>x</m>} in \(A \vect{x} = \lambda  \vect{x}\) is a column vector to the right
of A.  . It's not so common but still desirable in some situations to consider <q>left eigenvectors,</q> so Sage allows either one. {
} The \(right$\_$eigenmatrix\) method returns a pair of matrices.  The diagonal matrix, \(D\), with eigenvalues and the diagonalizing matrix, \(P\),
which is made up of columns that are eigenvectors corresponding to the eigenvectors of \(D\).

(D,P)=A.right$\_$eigenmatrix();(D,P)\\
     \texttt{ \\
}\texttt{ (\\
[6 0 0]  [ 1  1  1]\\
[0 4 0]  [ 2  0 -1]\\
[0 0 3], [ 1 -1  1]\\
)}



We should note here that \(P\) is not unique because even if an eigenspace has dimension one, any nonzero vector in that space will serve as an eigenvector.
  For that reason, the \(P\) generated by Sage isn't identical to the one generated by Mathematica. , but they both work.  Here we verify
the result for our Sage calculation.  Recall that an asterisk is used for matrix multiplication in Sage.

P.inverse()*A*P\\
=\texttt{ [6 0 0]\\
[0 4 0]\\
[0 0 3]}     



Here is a second matrix, again the same as we used with Mathematica. .

A2=Matrix(QQ,[[8,1,0],[1,5,1],[0,1,7]]);A2  \texttt{ \\
[8 1 0]\\
[1 5 1]\\
[0 1 7]}



Here we've already specified that the underlying system is the rational numbers.  Since the eigenvalues are not rational, Sage will revert to approximate
number by default. We'll just pull out the matrix of eigenvectors this time and display rounded entries.  Here the diagonalizing matrix looks very
different from the result from Mathematica. , but this is because he eigenvalues are not in the same order in the two calculations.   They
both diagonalize but with a different diagonal matrix.

P=A2.right$\_$eigenmatrix()[1]\\
P.numerical$\_$approx(digits=3)\\
       \texttt{ \\
[  1.00   1.00   1.00]\\
[ -3.65 -0.726  0.377]\\
[  1.38  -2.65  0.274]\\
}D=(P.inverse()*A2*P);D.numerical$\_$approx(digits=3)\texttt{ \\
[ 4.35 0.000 0.000]\\
[0.000  7.27 0.000]\\
[0.000 0.000  8.38]}


<exercises xml:id="exercises-12-4">
<title>Exercises for Section 12.4</title>



<exercisegroup>
<introduction><p>A Exercises</p></introduction>

<exercise number="1"><statement> (a) List three different eigenvectors of \(A=\left(
\begin{array}{cc}
 2 &amp; 1 \\
 2 &amp; 3 \\
\end{array}
\right)\), the matrix of Example 12.4.1, associated with the two eigenvalues 1 and 4.   Verify your results.



((b)  Choose one of the three eigenvectors corresponding to 1 and one of the three eigenvectors corresponding to 4, and show that the two chosen
vectors are linearly independent.
</statement></exercise>
<exercise number="2"><statement>  (a) Verify that \(E_1\) and \(E_2\) in Example 12.4.1 are vector spaces over <m>\mathbb{R}</m>.  Since they are also subsets of \(\mathbb{R}^2\),
they are called subvector-spaces, or subspaces for short, of \(\mathbb{R}^2\). Since these are subspaces consisting of eigenvectors, they are called
eigenspaces. </p></li>
<li><p> Use the definition of dimension in the previous section to find \(\dim \text E_1\) and \(\dim  E_2\) . Note that \(\textrm{ dim }E_1\textrm{ + dim
}E_2\textrm{  = dim }\mathbb{R}^2\). This is not a coincidence.
</statement></exercise>
<exercise number="3"><statement> (a) Verify that \(P^{-1} A P\) is indeed equal to \(\left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\),  as indicated in Example 12.4.2.</p></li>
<li><p> Choose \(P^{(1)}=\left(
\begin{array}{c}
 1 \\
 2 \\
\end{array}
\right)\) and \(P^{(2)}=\left(
\begin{array}{c}
 1 \\
 -1 \\
\end{array}
\right)\) and verify that the new value of \(P\) satisfies \(P^{-1} A P=\left(
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\)</p></li>
<li><p>  Take any two linearly independent eigenvectors of the matrix <m>A</m> of Example 12.4.2 and verify that \(P^{-1} A P\) is a diagonal matrix.
</statement></exercise>
<exercise number="4"><statement> (a) Let A be the matrix in Example 12.4.3 and \(P=\left(
\begin{array}{ccc}
 0 &amp; 1 &amp; 0 \\
 1 &amp; 0 &amp; 1 \\
 1 &amp; 0 &amp; 2 \\
\end{array}
\right)\).  Without doing any actual matrix multiplications, determine the value of \(P^{-1} A P\)</p></li>
<li><p>  If you choose the columns of P in the reverse order, what is \(P^{-1} A P\)?
</statement></exercise>
<exercise number="5"><statement>  Diagonalize the following, if possible:

<ol label="a">
<li><p>    \(\left(
\begin{array}{cc}
 1 &amp; 2 \\
 3 &amp; 2 \\
\end{array}
\right)\)(b)   \(\left(
\begin{array}{cc}
 -2 &amp; 1 \\
 -7 &amp; 6 \\
\end{array}
\right)\)   (c)  \(\left(
\begin{array}{cc}
 3 &amp; 0 \\
 0 &amp; 4 \\
\end{array}
\right)\)




</p></li>
<li><p>    \(\left(
\begin{array}{ccc}
 1 &amp; -1 &amp; 4 \\
 3 &amp; 2 &amp; -1 \\
 2 &amp; 1 &amp; -1 \\
\end{array}
\right)\)(e)   \(\left(
\begin{array}{ccc}
 6 &amp; 0 &amp; 0 \\
 0 &amp; 7 &amp; -4 \\
 9 &amp; 1 &amp; 3 \\
\end{array}
\right)\)(f)  \(\left(
\begin{array}{ccc}
 1 &amp; -1 &amp; 0 \\
 -1 &amp; 2 &amp; -1 \\
 0 &amp; -1 &amp; 1 \\
\end{array}
\right)\)
</statement></exercise>
<exercise number="6"><statement>   Diagonalize the following, if possible:

<ol label="a">
<li><p>    \(\left(
\begin{array}{cc}
 0 &amp; 1 \\
 1 &amp; 1 \\
\end{array}
\right)\)(b)   \(\left(
\begin{array}{cc}
 2 &amp; 1 \\
 4 &amp; 2 \\
\end{array}
\right)\)   $\quad \quad \quad $(c)  \(\left(
\begin{array}{cc}
 2 &amp; -1 \\
 1 &amp; 0 \\
\end{array}
\right)\)



 (d)   \(\left(
\begin{array}{ccc}
 1 &amp; 3 &amp; 6 \\
 -3 &amp; -5 &amp; -6 \\
 3 &amp; 3 &amp; 6 \\
\end{array}
\right)\)  (e)   \(\left(
\begin{array}{ccc}
 1 &amp; 1 &amp; 0 \\
 1 &amp; 0 &amp; 1 \\
 0 &amp; 1 &amp; 1 \\
\end{array}
\right)\)$\quad \quad \quad $(f)  \(\left(
\begin{array}{ccc}
 2 &amp; -1 &amp; 0 \\
 -1 &amp; 2 &amp; -1 \\
 0 &amp; -1 &amp; 2 \\
\end{array}
\right)\)


</exercisegroup>
<exercisegroup>
<introduction><p>B Exercises</p></introduction>
</statement></exercise>
<exercise number="7"><statement>  Let <m>A</m> and <m>P</m> be as in Example 12.4.3. Show that the columns of the matrix \(A P\) can be found by computing \(A P^{(1)}\),
\(A P^{(2)}\), $\ldots $, \(A P^{(n)}\). 
</statement></exercise>
<exercise number="8"><statement>  Prove that if \(P\) is an \(n\times n\) matrix and \(D\) is a diagonal matrix with diagonal entries \(d_1\), \(d_2\), $\ldots $, \(d_n\), then
\(P D\)is the matrix obtained from \(P\), but multiplying column <m>i</m> of <m>P</m> by \(d_i\), \(i = 1, 2, \ldots , n\).


</exercisegroup>
<exercisegroup>
<introduction><p>C Exercises</p></introduction>
</statement></exercise>
<exercise number="9"><statement>  (a)  There is an option to the Mathematica.  functions \(Eigenvectors\) and \(Eigenvalues\) called \(Cubics\) that will use the cubic
equation to find exact eigenvalues of a matrix like \(\left(
\begin{array}{ccc}
 8 &amp; 1 &amp; 0 \\
 1 &amp; 5 &amp; 1 \\
 0 &amp; 1 &amp; 7 \\
\end{array}
\right)\).   Use that option to find the exact eigenvalues of the matrix.  Diagonalize the matrix using the \(Cubics\) option and then convert
the result to a matrix of approximate numbers to compare your result with the approximate result we found in the Mathematica.  Note.
</statement>
</exercise>
</exercisegroup>
</exercises>
</section>

